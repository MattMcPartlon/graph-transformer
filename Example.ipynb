{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geometric_graph_transformer import GraphTransformer\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Structure\n",
    "## Encoder\n",
    "- standard node attention with pair bias\n",
    "- pair updates via node-wise outer product and triangle multiplication\n",
    "\n",
    "## Decoder\n",
    "- Node attention via IPA with distance-based attention for points\n",
    "- IPA layers do not use shared weights\n",
    "- No pair updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dim = 32 # hidden dimension of node features\n",
    "pair_dim = 32 # hidden dimension for pair features\n",
    "encoder_depth = 2\n",
    "decoder_depth = 1\n",
    "\n",
    "\"\"\"Node Feature Updates for encoder\n",
    "\n",
    "IPA not used in encoder for this example\n",
    "\"\"\"\n",
    "# defaults\n",
    "encoder_node_kwargs = dict(\n",
    "            dim_head= 32,\n",
    "            heads = 8,\n",
    "            bias= False,\n",
    ")\n",
    "\n",
    "\"\"\"Pair Feature Updates for Encoder\n",
    "\n",
    "node-outer product + triangle multiplication\n",
    "no triangle attention used (per empericism)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "encoder_pair_kwargs = dict(\n",
    "    heads = 4,\n",
    "    dim_head = 24,\n",
    "    dropout = 0,\n",
    "    tri_mul_dim = pair_dim,\n",
    "    do_checkpoint = True,\n",
    "    ff_mult = 2,\n",
    "    do_tri_mul = True,\n",
    "    do_tri_attn = False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Node Feature Updates for decoder\n",
    "\n",
    "IPA is used in decoder for this example\n",
    "\"\"\"\n",
    "# defaults\n",
    "decoder_node_kwargs = dict(\n",
    "    heads = 12,\n",
    "    dim_query_scalar = 16,\n",
    "    dim_query_point = 4,\n",
    "    dim_value_scalar = 16,\n",
    "    dim_value_point = 8,\n",
    "    pre_norm = True,\n",
    "    use_dist_attn = True,\n",
    ")\n",
    "\n",
    "\"\"\" No pair updates\n",
    "\"\"\"\n",
    "decoder_pair_kwargs = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "encoder = GraphTransformer(\n",
    "    node_dim = node_dim,\n",
    "    pair_dim = pair_dim,\n",
    "    depth = encoder_depth,\n",
    "    use_ipa = False,\n",
    "    node_update_kwargs = encoder_node_kwargs,\n",
    "    pair_update_kwargs = encoder_pair_kwargs,\n",
    "    share_weights = False,\n",
    ")\n",
    "\n",
    "# projections from encoder output to decoder input\n",
    "decoder_node_proj = nn.Sequential(\n",
    "    nn.LayerNorm(node_dim),\n",
    "    nn.Linear(node_dim,node_dim)\n",
    ")\n",
    "\n",
    "decoder_pair_proj = nn.Sequential(\n",
    "    nn.LayerNorm(pair_dim),\n",
    "    nn.Linear(pair_dim,pair_dim)\n",
    ")\n",
    "\n",
    "# decoder\n",
    "decoder = GraphTransformer(\n",
    "    node_dim = node_dim,\n",
    "    pair_dim = pair_dim,\n",
    "    depth = decoder_depth,\n",
    "    use_ipa = True, # use IPA in decoder\n",
    "    node_update_kwargs = decoder_node_kwargs,\n",
    "    pair_update_kwargs = decoder_pair_kwargs,\n",
    "    share_weights = False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder out shapes:\n",
      "    Node : torch.Size([1, 30, 32])\n",
      "    Pair: torch.Size([1, 30, 30, 32])\n",
      "\n",
      "Decoder out shapes:\n",
      "    Node : torch.Size([1, 30, 32])\n",
      "    Pair: torch.Size([1, 30, 30, 32])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "b,n = 1,30\n",
    "node_feats = torch.randn(b,n,node_dim)\n",
    "pair_feats = torch.randn(b,n,n,pair_dim)\n",
    "\n",
    "# example forward pass\n",
    "node_feats, pair_feats, *_ = encoder(\n",
    "    node_feats = node_feats, \n",
    "    pair_feats = pair_feats\n",
    ")\n",
    "\n",
    "print(f\"Encoder out shapes:\\n    Node : {node_feats.shape}\\n    Pair: {pair_feats.shape}\\n\")\n",
    "\n",
    "node_feats, pair_feats, rigids, _ = decoder(\n",
    "    node_feats = decoder_node_proj(node_feats),\n",
    "    pair_feats = decoder_pair_proj(pair_feats)\n",
    ")\n",
    "print(f\"Decoder out shapes:\\n    Node : {node_feats.shape}\\n    Pair: {pair_feats.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict BB coordinates from decoder output\n",
    "- Linearly project the node_feats and \n",
    "- apply rigids to place in local frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point shape : torch.Size([1, 30, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "point_dim = 4 # e.g. one dimension for each bb coord\n",
    "CA_posn = 1 # position of CA atom in predicted coords\n",
    "\n",
    "#predict points in local frame of each residue\n",
    "to_points = nn.Sequential(\n",
    "        nn.LayerNorm(node_dim),\n",
    "        nn.Linear(node_dim, 3 * point_dim, bias=False),\n",
    "        Rearrange(\"b n (d c) -> b n d c\", c=3),\n",
    "    )\n",
    "\n",
    "# predict from node features\n",
    "local_points = to_points(node_feats)\n",
    "\n",
    "# replace predicted CA with rigid translation (helps empirically)\n",
    "local_points[:, :, 1] = torch.zeros_like(local_points[:, :, 1])\n",
    "\n",
    "# place points in global frame by applying rigids\n",
    "global_points = rigids.apply(local_points)\n",
    "\n",
    "print(f\"point shape : {global_points.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Decoder with pair features and shared IPA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_pair_kwargs = encoder_pair_kwargs\n",
    "decoder = GraphTransformer(\n",
    "    node_dim = node_dim,\n",
    "    pair_dim = pair_dim,\n",
    "    depth = 2,\n",
    "    use_ipa = True, # use IPA in decoder\n",
    "    node_update_kwargs = decoder_node_kwargs,\n",
    "    pair_update_kwargs = encoder_pair_kwargs, # add pair updates\n",
    "    share_weights = True, # Share Weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder out shapes:\n",
      "    Node : torch.Size([1, 30, 32])\n",
      "    Pair: torch.Size([1, 30, 30, 32])\n",
      "\n",
      "Decoder out shapes:\n",
      "    Node : torch.Size([1, 30, 32])\n",
      "    Pair: torch.Size([1, 30, 30, 32])\n",
      "\n",
      "Aux. FAPE Loss: 0.9309613704681396\n"
     ]
    }
   ],
   "source": [
    "from rigids import Rigids\n",
    "\n",
    "#Need to provide native rigids for weight sharing\n",
    "native_bb = torch.randn(b,n,4,3)\n",
    "\n",
    "# Example input\n",
    "b,n = 1,30\n",
    "node_feats = torch.randn(b,n,node_dim)\n",
    "pair_feats = torch.randn(b,n,n,pair_dim)\n",
    "true_rigids = Rigids.RigidFromBackbone(native_bb)\n",
    "\n",
    "# example forward pass\n",
    "node_feats, pair_feats, *_ = encoder(\n",
    "    node_feats = node_feats, \n",
    "    pair_feats = pair_feats\n",
    ")\n",
    "\n",
    "print(f\"Encoder out shapes:\\n    Node : {node_feats.shape}\\n    Pair: {pair_feats.shape}\\n\")\n",
    "\n",
    "node_feats, pair_feats, rigids, fape_aux = decoder(\n",
    "    node_feats = decoder_node_proj(node_feats),\n",
    "    pair_feats = decoder_pair_proj(pair_feats),\n",
    "    true_rigids = true_rigids\n",
    ")\n",
    "print(f\"Decoder out shapes:\\n    Node : {node_feats.shape}\\n    Pair: {pair_feats.shape}\\n\")\n",
    "\n",
    "print(f\"Aux. FAPE Loss: {fape_aux}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder out shapes:\n",
      "    Node : torch.Size([1, 30, 32])\n",
      "    Pair: torch.Size([1, 30, 30, 32])\n",
      "\n",
      "Aux. FAPE Loss: 0.93241286277771\n"
     ]
    }
   ],
   "source": [
    "# initial rigids for decoy can also be passed to decoder \n",
    "decoy_coords = native_bb.clone()\n",
    "node_feats, pair_feats, rigids, fape_aux = decoder(\n",
    "    node_feats = decoder_node_proj(node_feats),\n",
    "    pair_feats = decoder_pair_proj(pair_feats),\n",
    "    true_rigids = true_rigids,\n",
    "    rigids = Rigids.RigidFromBackbone(decoy_coords)\n",
    ")\n",
    "print(f\"Decoder out shapes:\\n    Node : {node_feats.shape}\\n    Pair: {pair_feats.shape}\\n\")\n",
    "\n",
    "print(f\"Aux. FAPE Loss: {fape_aux}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
